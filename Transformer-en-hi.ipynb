{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68be5db-970c-47f8-bc0e-a34ebf5a26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "865da551-9b10-4a13-b757-4f839f20cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bindra.p/.local/lib/python3.8/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09e66d3c-a331-4ba5-8759-125efcdcafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d','',text)\n",
    "    text = re.sub(r'\\s+',' ',text)  #Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a17533-5b61-48ac-a605-8b6c1c96b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_datasets[\"train\"][\"translation\"]\n",
    "    \n",
    "eng_sen = []\n",
    "hin_sen = []\n",
    "\n",
    "# rows = [{\"en\": item[\"en\"], \"hi\": item[\"hi\"]} for item in data]\n",
    "for item in data:\n",
    "    eng_sen.append(item['en'])\n",
    "    hin_sen.append(item['hi'])\n",
    "    \n",
    "eng_sen = [preprocess(en) for en in eng_sen]\n",
    "hin_sen = [' ' + re.sub('[a-zA-Z]', '', preprocess(hi)) + ' ' for hi in hin_sen]\n",
    "\n",
    "#Remove duplicate sentences\n",
    "english_unique = set()\n",
    "english_sentences_temp = []\n",
    "hindi_sentences_temp = []\n",
    "l = len(eng_sen)\n",
    "for i in range(l):\n",
    "    if eng_sen[i] not in english_unique:\n",
    "        english_unique.add(eng_sen[i])\n",
    "        english_sentences_temp.append(eng_sen[i])\n",
    "        hindi_sentences_temp.append(hin_sen[i])\n",
    "\n",
    "eng_sen = english_sentences_temp\n",
    "hin_sen = hindi_sentences_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b3ba06-ea27-48b0-94a1-8d9b55dabdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044136 1044136\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['give your application an accessibility workout',\n",
       "  'accerciser accessibility explorer',\n",
       "  'the default plugin layout for the bottom panel'],\n",
       " [' अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें ',\n",
       "  ' एक्सेर्साइसर पहुंचनीयता अन्वेषक ',\n",
       "  ' निचले पटल के लिए डिफोल्ट प्लगइन खाका '])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(eng_sen), len(hin_sen))\n",
    "print()\n",
    "eng_sen[:3], hin_sen[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f2bbcf-e75d-4a84-81e8-f9dfd71b2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "total_sentences = 50\n",
    "maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fdb5c08-4b0b-4152-80a6-bd7b49d353e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = []\n",
    "hi_data = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for (en, hi) in zip(eng_sen, hin_sen):\n",
    "    l = min(len(en.split()), len(hi.split()))\n",
    "    if l <= maxlen:\n",
    "        en_data.append(en)\n",
    "        hi_data.append(hi)\n",
    "        cnt += 1\n",
    "    if cnt == total_sentences:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "898eeaed-018e-4640-855a-954f49610b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "def translator(text):\n",
    "  # function to translate english text to hindi\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", padding=True)\n",
    "  outputs = model.generate(input_ids)\n",
    "  decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  \n",
    "  return decoded_text\n",
    "\n",
    "#text you want translate\n",
    "texts = en_data\n",
    "hi_translated = []\n",
    "\n",
    "for text in texts:\n",
    "    hi_translated.append(translator(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9123195a-9efc-40a5-aff2-570996f76089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ed9597c-a255-4b90-95d6-0b803bd11109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(sentence):\n",
    "    # Remove punctuation and special tokens\n",
    "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return cleaned_sentence\n",
    "cleaned_predictions = [remove_special_tokens(sentence) for sentence in hi_translated]\n",
    "hi_translated = [nltk.word_tokenize(sentence) for sentence in cleaned_predictions]\n",
    "\n",
    "hi_data = [nltk.word_tokenize(sentence) for sentence in hi_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0bfbd7b-0271-4802-a4db-d25d15f5fc13",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('तरछ',)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-059fc7ad9338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msmoothie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# bleu_score = corpus_bleu(hi_data_cleaned, hi_translated_cleaned, smoothing_function=smoothie)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_translated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBLEU score =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2022.01/lib/python3.8/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# denominator for the corpus-level modified precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mp_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mp_numerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mp_denominators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2022.01/lib/python3.8/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mmodified_precision\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     clipped_counts = {\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mngram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     }\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2022.01/lib/python3.8/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Assigns the intersection between hypothesis and references' counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     clipped_counts = {\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mngram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     }\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('तरछ',)"
     ]
    }
   ],
   "source": [
    "smoothie = SmoothingFunction().method1\n",
    "# bleu_score = corpus_bleu(hi_data_cleaned, hi_translated_cleaned, smoothing_function=smoothie)\n",
    "bleu_score = corpus_bleu(hi_data, hi_translated, smoothing_function=smoothie)\n",
    "\n",
    "print(\"\\nBLEU score =\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b32320a-be3c-4d8b-b2be-5468f004a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['अपन', 'अनपरयग', 'क', 'पहचनयत', 'वययम', 'क', 'लभ', 'द'],\n",
       " ['एकसरसइसर', 'पहचनयत', 'अनवषक'],\n",
       " ['नचल', 'पटल', 'क', 'लए', 'डफलट', 'पलगइन', 'खक'],\n",
       " ['ऊपर', 'पटल', 'क', 'लए', 'डफलट', 'पलगइन', 'खक'],\n",
       " ['उन',\n",
       "  'पलगइन',\n",
       "  'क',\n",
       "  'सच',\n",
       "  'जनह',\n",
       "  'डफलट',\n",
       "  'रप',\n",
       "  'स',\n",
       "  'नषकरय',\n",
       "  'कय',\n",
       "  'गय',\n",
       "  'ह'],\n",
       " ['हइलइट', 'अवध'],\n",
       " ['पहचनय', 'आसध', 'नड', 'क', 'चनत', 'समय', 'हइलइट', 'बकस', 'क', 'अवध'],\n",
       " ['समत', 'बरडर', 'क', 'रग', 'क', 'हइलइट', 'कर'],\n",
       " ['हइलइट', 'कए', 'गए', 'समत', 'क', 'रग', 'और', 'अपरदरशत'],\n",
       " ['भरई', 'क', 'रग', 'क', 'हइलइट', 'कर'],\n",
       " ['हइलइट', 'कय', 'गय', 'भरई', 'क', 'रग', 'और', 'परदरशत'],\n",
       " ['एक', 'समनय', 'बरउजर'],\n",
       " ['इस',\n",
       "  'समय',\n",
       "  'जस',\n",
       "  'परपत',\n",
       "  'कय',\n",
       "  'गय',\n",
       "  'ह',\n",
       "  'उसक',\n",
       "  'वभनन',\n",
       "  'वधय',\n",
       "  'मथड',\n",
       "  'म',\n",
       "  'बरउज',\n",
       "  'कर'],\n",
       " ['नज', 'गण', 'क', 'छपए'],\n",
       " ['वध'],\n",
       " ['गण'],\n",
       " ['मन'],\n",
       " ['आईपयथन', 'कसल'],\n",
       " ['इस',\n",
       "  'समय',\n",
       "  'चन',\n",
       "  'गए',\n",
       "  'एकससबल',\n",
       "  'स',\n",
       "  'कम',\n",
       "  'लन',\n",
       "  'क',\n",
       "  'लए',\n",
       "  'अतरकरयतमक',\n",
       "  'कनसल'],\n",
       " ['घटन', 'मनटर'],\n",
       " ['घटनओ', 'क', 'मनटर', 'कर'],\n",
       " ['सफ', 'कर', 'चयन'],\n",
       " ['सभ'],\n",
       " ['चन', 'गए', 'अनपरयग'],\n",
       " ['चन', 'गए', 'एकससबल'],\n",
       " ['सरत'],\n",
       " ['चन',\n",
       "  'गए',\n",
       "  'परकर',\n",
       "  'और',\n",
       "  'सरत',\n",
       "  'स',\n",
       "  'घटनए',\n",
       "  'जसजस',\n",
       "  'घटत',\n",
       "  'हत',\n",
       "  'ह',\n",
       "  'उनह',\n",
       "  'दरशत',\n",
       "  'ह'],\n",
       " ['अतम', 'परवषट', 'घटन', 'क', 'हइलइट', 'कर'],\n",
       " ['घटन', 'रकरड', 'करन', 'आरभ', 'कर'],\n",
       " ['घटन', 'रजनमच', 'मटओ'],\n",
       " ['कई', 'ववरण', 'नह'],\n",
       " ['वरणन'],\n",
       " ['श'],\n",
       " ['पहच', 'यगय'],\n",
       " ['करय'],\n",
       " ['करय', 'सच'],\n",
       " ['आईड'],\n",
       " ['औजर', 'सआर', 'n'],\n",
       " ['ससकरण'],\n",
       " ['अनपरयग'],\n",
       " ['सगरह'],\n",
       " ['तरछ', 'टइप', 'हमर'],\n",
       " ['सपकष', 'सथत'],\n",
       " ['आकर'],\n",
       " ['वजट'],\n",
       " ['सतर'],\n",
       " ['ma', 'z', 'अनकरम'],\n",
       " ['अलफ'],\n",
       " ['नरपकष', 'सथत'],\n",
       " ['कदरusa', 'kgm']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d65c8-791c-4a6e-9f9c-389c690b79af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
